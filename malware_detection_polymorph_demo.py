
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
import random
import matplotlib.pyplot as plt
from multiprocessing import Pool
import uuid
import os
import shap
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Suppress TensorFlow logging
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# Simulate a malware dataset (replace with real dataset like PE headers or API calls)
np.random.seed(42)
num_samples = 10000
num_features = 200  # Simulating byte sequences, API calls, etc.
X = np.random.rand(num_samples, num_features, 1)  # 1D sequences
y = np.random.randint(0, 2, num_samples)  # Binary: 0 (benign), 1 (malicious)
y = tf.keras.utils.to_categorical(y, 2)
train_split = int(0.8 * num_samples)
X_train, X_test = X[:train_split], X[train_split:]
y_train, y_test = y[:train_split], y[train_split:]

# Apply PCA for feature selection
scaler = StandardScaler()
X_train_flat = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))
X_test_flat = scaler.transform(X_test.reshape(X_test.shape[0], -1))
pca = PCA(n_components=50)  # Reduce to 50 features
X_train_pca = pca.fit_transform(X_train_flat).reshape(-1, 50, 1)
X_test_pca = pca.transform(X_test_flat).reshape(-1, 50, 1)

# ModelConfig class for genetic algorithm
class ModelConfig:
    def __init__(self):
        self.conv_layers = []
        self.lstm_units = random.choice([32, 64, 128])
        self.dense_layers = []
        self.optimizer = random.choice(['adam', 'rmsprop'])
        self.learning_rate = random.choice([0.0001, 0.001, 0.01])
        self.batch_size = random.choice([32, 64, 128])
        self.id = str(uuid.uuid4())
    
    def add_conv_layer(self):
        filters = random.choice([16, 32, 64])
        kernel_size = random.choice([3, 5])
        activation = random.choice(['relu', 'elu'])
        pooling = random.random() < 0.5
        dropout = random.random() < 0.3
        batch_norm = random.random() < 0.3
        self.conv_layers.append({
            'filters': filters,
            'kernel_size': kernel_size,
            'activation': activation,
            'pooling': pooling,
            'dropout': dropout,
            'batch_norm': batch_norm
        })
    
    def add_dense_layer(self):
        units = random.choice([64, 128, 256])
        activation = random.choice(['relu', 'elu'])
        dropout = random.random() < 0.4
        batch_norm = random.random() < 0.3
        self.dense_layers.append({
            'units': units,
            'activation': activation,
            'dropout': dropout,
            'batch_norm': batch_norm
        })

# Create a hybrid CNN-LSTM model
def create_model(config):
    model = models.Sequential()
    model.add(layers.Input(shape=(50, 1)))
    
    for conv in config.conv_layers:
        model.add(layers.Conv1D(
            filters=conv['filters'],
            kernel_size=conv['kernel_size'],
            activation=conv['activation'],
            padding='same'
        ))
        if conv['batch_norm']:
            model.add(layers.BatchNormalization())
        if conv['pooling']:
            model.add(layers.MaxPooling1D(2))
        if conv['dropout']:
            model.add(layers.Dropout(0.25))
    
    model.add(layers.LSTM(config.lstm_units, return_sequences=False))
    model.add(layers.Flatten())
    
    for dense in config.dense_layers:
        model.add(layers.Dense(dense['units'], activation=dense['activation']))
        if dense['batch_norm']:
            model.add(layers.BatchNormalization())
        if dense['dropout']:
            model.add(layers.Dropout(0.5))
    
    model.add(layers.Dense(2, activation='softmax'))
    
    optimizer = {
        'adam': tf.keras.optimizers.Adam(learning_rate=config.learning_rate),
        'rmsprop': tf.keras.optimizers.RMSprop(learning_rate=config.learning_rate)
    }[config.optimizer]
    
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Evaluate a model
def evaluate_model(config):
    model = create_model(config)
    history = model.fit(
        X_train_pca, y_train,
        epochs=5,
        batch_size=config.batch_size,
        validation_split=0.2,
        verbose=0
    )
    test_loss, test_accuracy = model.evaluate(X_test_pca, y_test, verbose=0)
    return config, test_accuracy, model

# Mutate a model configuration
def mutate_config(config):
    new_config = ModelConfig()
    new_config.conv_layers = config.conv_layers.copy()
    if random.random() < 0.3 and len(new_config.conv_layers) < 3:
        new_config.add_conv_layer()
    elif random.random() < 0.2 and len(new_config.conv_layers) > 1:
        new_config.conv_layers.pop(random.randint(0, len(new_config.conv_layers)-1))
    else:
        for layer in new_config.conv_layers:
            if random.random() < 0.3:
                layer['filters'] = random.choice([16, 32, 64])
            if random.random() < 0.3:
                layer['kernel_size'] = random.choice([3, 5])
            if random.random() < 0.3:
                layer['activation'] = random.choice(['relu', 'elu'])
            if random.random() < 0.3:
                layer['pooling'] = random.random() < 0.5
            if random.random() < 0.3:
                layer['dropout'] = random.random() < 0.3
            if random.random() < 0.3:
                layer['batch_norm'] = random.random() < 0.3
    
    new_config.dense_layers = config.dense_layers.copy()
    if random.random() < 0.3 and len(new_config.dense_layers) < 2:
        new_config.add_dense_layer()
    elif random.random() < 0.2 and len(new_config.dense_layers) > 1:
        new_config.dense_layers.pop(random.randint(0, len(new_config.dense_layers)-1))
    else:
        for layer in new_config.dense_layers:
            if random.random() < 0.3:
                layer['units'] = random.choice([64, 128, 256])
            if random.random() < 0.3:
                layer['activation'] = random.choice(['relu', 'elu'])
            if random.random() < 0.3:
                layer['dropout'] = random.random() < 0.4
            if random.random() < 0.3:
                layer['batch_norm'] = random.random() < 0.3
    
    if random.random() < 0.3:
        new_config.lstm_units = random.choice([32, 64, 128])
    if random.random() < 0.3:
        new_config.optimizer = random.choice(['adam', 'rmsprop'])
    if random.random() < 0.3:
        new_config.learning_rate = random.choice([0.0001, 0.001, 0.01])
    if random.random() < 0.3:
        new_config.batch_size = random.choice([32, 64, 128])
    
    return new_config

# Genetic algorithm
def run_genetic_algorithm(population_size=5, generations=3):
    population = [ModelConfig() for _ in range(population_size)]
    for config in population:
        num_conv = random.randint(1, 3)
        num_dense = random.randint(1, 2)
        for _ in range(num_conv):
            config.add_conv_layer()
        for _ in range(num_dense):
            config.add_dense_layer()
    
    best_accuracies = []
    best_model = None
    best_config = None
    
    for generation in range(generations):
        print(f"\nGeneration {generation + 1}")
        
        with Pool() as pool:
            results = pool.map(evaluate_model, population)
        
        results.sort(key=lambda x: x[1], reverse=True)
        population = [config for config, _, _ in results]
        accuracies = [acc for _, acc, _ in results]
        models = [model for _, _, model in results]
        
        print(f"Best Accuracy: {accuracies[0]:.4f}")
        best_accuracies.append(accuracies[0])
        if accuracies[0] > max(best_accuracies[:-1], default=0):
            best_model = models[0]
            best_config = population[0]
        
        new_population = population[:2]
        while len(new_population) < population_size:
            parent = random.choice(population[:3])
            child = mutate_config(parent)
            new_population.append(child)
        
        population = new_population
    
    return best_accuracies, best_model, best_config

# Run the genetic algorithm
best_accuracies, best_model, best_config = run_genetic_algorithm()

# Explain model predictions with SHAP
explainer = shap.DeepExplainer(best_model, X_train_pca[:100])
shap_values = explainer.shap_values(X_test_pca[:100])
shap.summary_plot(shap_values[1], X_test_pca[:100], plot_type="bar", show=False)
plt.savefig('shap_feature_importance.png')
plt.close()

# Plot accuracy evolution
plt.plot(range(1, len(best_accuracies) + 1), best_accuracies, marker='o')
plt.xlabel('Generation')
plt.ylabel('Best Test Accuracy')
plt.title('Evolution of Model Accuracy for Malware Detection')
plt.grid(True)
plt.savefig('malware_detection_accuracy.png')
plt.close()

# Save the best model
best_model.save("best_malware_detection_model.h5")
```


# -pk
